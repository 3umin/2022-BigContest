# ??? (1)

[is_applied 예측_최종진행예정.ipynb]((1)%20d82ea6124879491d8f29f61d12819f3c/is_applied_%25EC%2598%2588%25EC%25B8%25A1_%25EC%25B5%259C%25EC%25A2%2585%25EC%25A7%2584%25ED%2596%2589%25EC%2598%2588%25EC%25A0%2595.ipynb)

- robust 스케일링 한 데이터로 전체 성능 비교하는중 - 수민

언더샘플링 비율 0.4일때

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Precision | Recall |
| --- | --- | --- | --- | --- | --- |
| 나이브베이즈 | 0.7082 | 0.9341 | 0.0191 | 0.0118 | 0.0489 |
| 랜덤포레스트 | 0.9952 | 0.8807 | 0.3037 | 0.4808 | 0.2219 |
| 로지스틱 | 0.7142 | 0.9459 | 0.0 | 0.0 | 0.0 |
| 의사결정나무 | 0.9952 | 0.7734 | 0.1948 | 0.5068 | 0.1206 |
| XGBoost | 0.7763 | 0.8850 | 0.2890 | 0.4320 | 0.2171 |
| LGBM | 0.7687 | 0.8851 | 0.2783 | 0.4095 | 0.2107 |
| CatBoost | 0.7814 | 0.8849 | 0.2948 | 0.4448 | 0.2204 |
| AdaBoost | 0.7522 | 0.8911 | 0.2440 | 0.3248 | 0.1953 |
| SVC | 0.5773 | 0.6258 | 0.1103 | 0.4288 | 0.0632 |
| Bagging | 0.9786 | 0.86848 | 0.2671 | 0.4433 | 0.1912 |
| GBD | 0.7602 | 0.8979 | 0.2627 | 0.3362 | 0.2156 |
| SGD | 0.2857 | 0.0540 | 0.1026 | 1 | 0.0540 |

언더샘플링 비율 0.6일때

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Precision | Recall |
| --- | --- | --- | --- | --- | --- |
| 나이브베이즈 | 0.6099 | 0.9086 | 0.0331 | 0.0289 | 0.0386 |
| 랜덤포레스트 | 0.9957 | 0.8306 | 0.2777 | 0.6022 | 0.1804 |
| 로지스틱 | 0.6249 | 0.9459 | 0.0 | 0.0 | 0.0 |
| 의사결정나무 | 0.9957 | 0.7236 | 0.1832 | 0.5732 | 0.1090 |
| XGBoost | 0.7610 | 0.8291 | 0.2662 | 0.5732 | 0.1734 |
| LGBM | 0.7350 | 0.8249 | 0.2567 | 0.5592 | 0.1666 |
| CatBoost | 0.7510 | 0.8291 | 0.2701 | 0.5849 | 0.1756 |
| AdaBoost | 0.7097 | 0.8254 | 0.2315 | 0.4862 | 0.1519 |
| SVC | 0.6320 | 0.8411 | 0.1338 | 0.2268 | 0.0949 |
| Bagging | 0.9792 | 0.8240 | 0.2492 | 0.5402 | 0.1620 |
| GBD | 0.7227 | 0.8364 | 0.2495 | 0.5028 | 0.1659 |
| SGD | 0.6248 | 0.9454 | 0.0005 | 0.0002 | 0.0315 |

언더 0.4, 오버 0.8 (최종 96만 / 77만개)

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Precision | Recall |
| --- | --- | --- | --- | --- | --- |
| 나이브베이즈 | 0.7081 | 0.9337 | 0.0195 | 0.0122 | 0.0487 |
| 랜덤포레스트 | 0.9950 | 0.8807 | 0.3036 | 0.4808 | 0.2218 |
| 로지스틱 | 0.7142 | 0.9459 | 0.0 | 0.0 | 0.0 |
| 의사결정나무 | 0.9951 | 0.7736 | 0.1948 | 0.5065 | 0.1206 |
| XGBoost | 0.7766 | 0.8850 | 0.2885 | 0.4308 | 0.2168 |
| LGBM | 0.7687 | 0.8855 | 0.2782 | 0.4078 | 0.2111 |
| CatBoost | 0.7813 | 0.8845 | 0.2946 | 0.4458 | 0.2199 |
| AdaBoost | 0.7521 | 0.8902 | 0.2436 | 0.3266 | 0.1942 |
| SVC | 0.7179 | 0.9427 | 0.0441 | 0.0244 | 0.2263 |
| Bagging | 0.9788 | 0.8686 | 0.2673 | 0.4432 | 0.1914 |
| GBD | 0.7602 | 0.8991 | 0.2628 | 0.3325 | 0.2173 |
| SGD | 0.2857 | 0.0540 | 0.1026 | 1.0 | 0.0540 |

```
 0   gender                  float64
 1   income_type             int32
 2   houseown_type           int32
 3   purpose                 int32
 4   bank_id                 int64
 5   loan_limit              float64
 6   loan_rate               float64
 7   age                     float64
 8   yearly_income_standard  float64
 9   limit_desired           float64
 10  var                     int64
```

언더 샘플링 0.2

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 나이브베이즈 | 0.9363 | 0.9363 | 0.0174 | 0.0524 | 0.0104 |
| 랜덤포레스트 | 0.972 | 0.926 | 0.3181 | 0.317 | 0.3194 |
| 로지스틱 | 0.946 | 0.946 | 0 | 0 | 0 |
| 의사결정나무 | 0.911 | 0.8457 | 0.2269 | 0.1556 | 0.4185 |
| XGBoost | 0.9304 | 0.93 | 0.3 | 0.3261 | 0.2782 |
| LGBM | 0.933 | 0.9327 | 0.2751 | 0.3297 | 0.236 |
| CatBoost | 0.93063 | 0.93 | 0.3104 | 0.3298 | 0.2932 |
| AdaBoost | 0.933 | 0.9329 | 0.185 | 0.2697 | 0.1408 |
| SVC | 0.9438 | 0.9438 | 0.0272 | 0.2154 | 0.0145 |
| Bagging | 0.959 | 0.9143 | 0.2924 | 0.2641 | 0.3274 |
| GBD | 0.93612 | 0.93598 | 0.2264 | 0.3267 | 0.1732 |
| SGD | 0.94546 | 0.9454 | 0.0006 | 0.0315 | 0.0003 |

언더샘플링 0.4

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 나이브베이즈 | 0.9198 | 0.92 | 0.0278 | 0.04052 | 0.0211587 |
| 랜덤포레스트 | 0.9227 | 0.8839 | 0.3223 | 0.23548 | 0.510488 |
| 로지스틱 | 0.94592 | 0.94592 | 0 | 0 | 0 |
| 의사결정나무 | 0.8374 | 0.785717 | 0.2118 | 0.1322 | 0.5323 |
| XGBoost | 0.8853 | 0.8844 | 0.3175 | 0.2332 | 0.4968 |
| LGBM | 0.8878 | 0.8873 | 0.3059 | 0.2294 | 0.4589 |
| CatBoost | 0.8849 | 0.88366 | 0.3239 | 0.2362 | 0.5153 |
| AdaBoost | 0.88664 | 0.88638 | 0.255 | 0.1975 | 0.3595 |

age, gender, var 드랍한 이후 언더샘플링 0.4

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.90259 | 0.8653 | 0.2925 | 0.20429 | 0.51496 |
| XGBoost | 0.88447 | 0.88366 | 0.315 | 0.2311 | 0.4947 |
| LGBM | 0.88785 | 0.8875 | 0.3046 | 0.2288 | 0.4556 |
| CatBoost | 0.884 | 0.883 | 0.32 | 0.2333 | 0.5087 |

- yearly_income이랑 desired_amount 스케일링 방법 standardScaler로 바꾼 버전 필요 -석범
    
    
    | 모델 | 테스트셋 정확도 | 훈련셋 정확도 | 테스트셋 F1 | recall  | precision |
    | --- | --- | --- | --- | --- | --- |
    | 나이브베이즈 | 0.9400 | 0.9400 | 0.0139 |  |  |
    | 랜덤포레스트 | 0.9432 | 0.9947 | 0.1449 |  |  |
    | 로지스틱 | 0.9459 | 0.9459 | 0.00 | 0 | 0 |
    | 의사결정나무 | 0.9114 | 0.947 | 0.2122 | 0.2045 | 0.2206 |
    | XGBoost | 0.946 | 0.9461 | 0.033 | 0.5224 | 0.01 |
    | LGBM | 0.9459 | 0.9459 | 0.01 | 0.5432 | 0.0057 |
    | CatBoost | 0.9459 | 0.9467 | 0.049 | 0.5126 | 0.02 |
    | AdaBoost | 0.9457 | 0.9457 | 0.01 | 0.3731 | 0.0059 |
    | SVC | 0.9458 | 0.9458 | 0.0003 | 0.0817 | 0.0002 |
    | Bagging | 0.9409 | 0.9872 | 0.1723 | 0.3559 | 0.1137 |
    | GBD | 0.9459 | 0.9459 | 0.01 | 0.4938 | 0.0051 |
    | SGD | 0.9454 | 0.9454 | 0.0006 | 0.0315 | 0.0003 |

- yearly_income이랑 승재가 만든 새 변수 Robust로 스케일링해서 학습하는 버전 필요 -우성

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 나이브베이즈 | 0.9397 | 0.9397 | 0.0145 |  |  |
| 랜덤포레스트 | 0.9947 | 0.9428 | 0.1472 |  |  |
| 로지스틱 | 0.9459 | 0.9459 | 0.001 |  |  |
| 의사결정나무 | 0.9948 | 0.9111 | 0.2093 |  |  |
| XGBoost | 0.9461 | 0.9460 | 0.0325 |  |  |
| LGBM | 0.9460 | 0.9460 | 0.0115 |  |  |
| CatBoost | 0.9468 | 0.9460 | 0.0503 |  |  |
| AdaBoost | 0.9457 | 0.9457 | 0.0114 |  |  |
| SVC | 0.2799 | 0.2798 | 0.0798 |  |  |
| Bagging | 0.9872 | 0.9409 | 0.1704 |  |  |

- 연령과 성별을 독립변수에서 제외한 모델 필요(1. yearly_income, desired_amount를 robust)

- 연령과 성별을 독립변수에서 제외한 모델 필요(2. yearly_income, desired_amount를 stand)

- 연령과 성별을 독립변수에서 제외한 모델 필요(3. yearly_income, 승재 변수 robust)

- 연령과 성별을 독립변수에서 제외한 모델 필요(4. yearly_income, 승재 변수 stand)

```python
train_set['limit_desired'] = train_set['loan_limit'] - train_set['desired_amount']
from sklearn.preprocessing import StandardScaler
import numpy as np

scaler = StandardScaler()
train_set['limit_desired_stand'] = scaler.fit_transform(np.array(train_set['limit_desired']).reshape(-1, 1))
train_set.drop(['limit_desired'], axis=1, inplace=True)
```

F1 score 올릴 방법 찾기

[http://aispiration.com/model/model-class-imbalance.html](http://aispiration.com/model/model-class-imbalance.html) 방법으로 oversampling이나 undersampling 등 필요

[https://cori.tistory.com/168](https://cori.tistory.com/168)

```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=0)
X_train_over, y_train_over = smote.fit_resample(X_train, y_train)
print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트:', X_train.shape, y_train.shape)
print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트:', X_train_over.shape, y_train_over.shape)
print('SMOTE 적용 후 레이블 값 분포: \n', pd.Series(y_train_over).value_counts())
```

```python
# 자동 결측치 대체
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
imp = IterativeImputer(max_iter=10, random_state=0)
```

```python
data.existing_loan_cnt.fillna(0, inplace = True)
data.existing_loan_amt.fillna(0, inplace = True)
data.personal_rehabilitation_yn.fillna(0, inplace = True)
bank랑 product 드랍 안시키고 대출금액 로버스트 안시킨 값

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
train = train_set.copy()
train = IterativeImputer(random_state = 77).fit_transform(train)
train = pd.DataFrame(train)
train.columns = train_set.columns
train
```

언더 샘플링 0.4

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.9246 | 0.8931 | 0.3967 | 0.28554 | 0.64957 |
| XGBoost | 0.8843 | 0.88332 | 0.3711 | 0.2619 | 0.6365 |
| LGBM | 0.88072 | 0.88021 | 0.3601 | 0.2532 | 0.6234 |
| CatBoost | 0.89135 | 0.89 | 0.3917 | 0.2795 | 0.6544 |
- bank, product 드랍, 대출금액 로버스트화, 언더샘플링 0.4

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.924967 | 0.894738 | 0.4032 | 0.29076 | 0.657642 |
| XGBoost | 0.878448 | 0.8773016 | 0.3506 | 0.2456 | 0.6125 |
| LGBM | 0.876378 | 0.87573 | 0.3439 | 0.2407 | 0.6022 |
| CatBoost | 0.887162 | 0.88596 | 0.3726 | 0.2652 | 0.6260 |
- 다시

[전처리 다시 시작.ipynb]((1)%20d82ea6124879491d8f29f61d12819f3c/%25EC%25A0%2584%25EC%25B2%2598%25EB%25A6%25AC_%25EB%258B%25A4%25EC%258B%259C_%25EC%258B%259C%25EC%259E%2591.ipynb)

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.925497 | 0.89517 | 0.4075 | 0.2947 | 0.66 |
| XGBoost | 0.87759 | 0.8765 | 0.3526 | 0.2471 | 0.6155 |
| LGBM | 0.876 | 0.875548 | 0.3473 | 0.2433 | 0.6064 |
| CatBoost | 0.8878 | 0.8867 | 0.3769 | 0.2694 | 0.6272 |

[승부는 미궁속으로.ipynb]((1)%20d82ea6124879491d8f29f61d12819f3c/%25EC%258A%25B9%25EB%25B6%2580%25EB%258A%2594_%25EB%25AF%25B8%25EA%25B6%2581%25EC%2586%258D%25EC%259C%25BC%25EB%25A1%259C.ipynb)

- 언더샘플링 0.4, stand, 결측치 전부 imputer로 대체

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.9553 | 0.9357 | 0.5660 | 0.4457 | 0.7749 |
| XGB | 0.9317 | 0.9311 | 0.5462 | 0.4244 | 0.7658 |
| LGBM | 0.9045 | 0.9039 | 0.4342 | 0.3185 | 0.6817 |
| CatBoost | 0.9508 | 0.9503 | 0.6423 | 0.5260 | 0.8247 |
- 언더샘플링 0.4, robust, 결측치 전부 imputer로 대체

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.9555 | 0.9362 | 0.5687 | 0.4483 | 0.7774 |
| XGB | 0.9318 | 0.9310 | 0.5452 | 0.4239 | 0.7640 |
| LGBM | 0.9043 | 0.9037 | 0.4334 | 0.3179 | 0.6807 |
| CatBoost | 0.9511 | 0.9506 | 0.6439 | 0.5280 | 0.8248 |

personal_rehabilitation_yn, personal_rehabilitation_complete_yn을 imputer을 활용해서 0~1 사이 임의 값으로 대체한 후 활용한 결과 가장 좋았음

→ 발표할 때 그냥 ~~~~한 정도 라는 이유로 변수를 0~1 사이로 세팅한 것이라 말하고, 이대로 활용해도 될 것 같음 이거보다 좋은 방법이 없다면

```
 0   gender                               float64
 1   credit_score                         float64
 2   income_type                          float64
 3   employment_type                      float64
 4   houseown_type                        float64
 5   purpose                              float64
 6   personal_rehabilitation_yn           float64
 7   personal_rehabilitation_complete_yn  float64
 8   existing_loan_cnt                    float64
 9   loan_rate                            float64
 10  is_applied                           float64
 11  age                                  float64
 12  yearly_income_robust                 float64
 13  ~~yearly_income_stand~~                  float64
 14  limit_desired_robust                 float64
 15  ~~limit_desired_stand~~                  float64
 16  existing_loan_amt_robust             float64
 17  ~~existing_loan_amt_stand~~              float64
 18  var                                  float64
```

purpose 빼고 더미 & 개인회생 처리 1차(drop) & 결측치 자동 승재 기본값

| 모델 | 테스트셋 정확도 | 훈련셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.900 | 0.996 | 0.4298 | 0.3174 | 0.6655 |
| XGB | 0.8857 | 0.8322 | 0.385 | 0.2767 | 0.6328 |
| LGBM | 0.882 | 0.819 | 0.3715 | 0.2665 | 0.6130 |
| CatBoost | 0.8931 | 0.8405 | 0.4033 | 0.2947 | 0.6386 |

purpose 빼고 더미 & 개인회생 처리 2차(non drop) & 결측치 자동 승재 기본값

| 모델 | 테스트셋 정확도 | 훈련셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.9104 | 0.9986 | 0.4571 | 0.34 | 0.6974 |
| XGB | 0.8911 | 0.8343 | 0.3890 | 0.2793 | 0.6407 |
| LGBM | 0.8839 | 0.8214 | 0.367 | 0.2603 | 0.6220 |
| CatBoost | 0.901 | 0.849 | 0.4214 | 0.3085 | 0.6642 |

전부 라벨 인코딩 (nondrop) & 결측치 오토 

| 모델 | 테스트셋 정확도 | 훈련셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.9081 | 0.9991 | 0.4487 | 0.3322 | 0.6911 |
| XGB | 0.9011 | 0.8504 | 0.4266 | 0.3108 | 0.6801 |
| LGBM | 0.8897 | 0.8292 | 0.3852 | 0.2758 | 0.6387 |
| CatBoost | 0.9254 | 0.8845 | 0.5184 | 0.3982 | 0.7424 |

전부 더미 (nondrop)(var 도 더미)  & 결측치 오토

| 모델 | 테스트셋 정확도 | 훈련셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.9080 | 0.9986 | 0.4463 | 0.3309 | 0.6851 |
| XGB | 0.8891 | 0.8308 | 0.3816 | 0.2731 | 0.6333 |
| LGBM | 0.8823 | 0.8177 | 0.3605 | 0.2553 | 0.6130 |
| CatBoost | 0.890 | 0.8454 | 0.4132 | 0.3013 | 0.6572 |
|  |  |  |  |  |  |

승부는+내꺼(ver.1)(product 라벨인코딩으로 포함 x)

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.9206 |  | 0.4980 | 0.3785 | 0.7277 |
| XGB | 0.90442 | 0.9054 | 0.4347 | 0.3195 | 0.6794 |
| LGBM | 0.8879 | 0.8886 | 0.3789 | 0.2705 | 0.6321 |
| CatBoost | 0.9435 | 0.9442 | 0.5962 | 0.4859 | 0.7711 |

승부는+내꺼(ver.2)(product 라벨인코딩으로 포함)

| 모델 | 훈련셋 정확도 | 테스트셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 |  |  |  |  |  |
| XGB |  |  |  |  |  |
| LGBM |  |  |  |  |  |
| CatBoost |  |  |  |  |  |

| 모델 | 테스트셋 정확도 | 훈련셋 정확도 | 테스트셋 F1 | Recall | Precision |
| --- | --- | --- | --- | --- | --- |
| 랜덤포레스트 | 0.9067 | 0.9987 | 0.4412 | 0.3263 | 0.6809 |
| XGB | 0.8886 | 0.8307 | 0.3817 | 0.2727 | 0.6354 |
| LGBM | 0.8825 | 0.8179 | 0.3604 | 0.2554 | 0.6122 |
| CatBoost | 0.8985 | 0.5451 | 0.4116 | 0.2998 | 0.6561 |

```python
personal_rehabilitation_yn=0 -> 개인회생자가 아닌경우
personal_rehabilitation_yn=1 -> 개인회생자인 경우
personal_rehabilitation_yn=2 -> 개인회생자 결측치(NaN)인경우

personal_rehabilitation_complete_yn=0 -> 개인회생자이면서 납입중인 경우
personal_rehabilitation_complete_yn=1 -> 개인회생자이면서 납입완료인 경우
personal_rehabilitation_complete_yn=2 -> 납입중도 아니고 납입완료도 아닌경우

personal_rehabilitation_yn=0 & personal_rehabilitation_complete_yn=0
(개인회생자가 아니면서 납입중) # check
-> 1691187
personal_rehabilitation_yn=0 & personal_rehabilitation_complete_yn=1
(개인회생자가 아니면서 납입완료)
-> 16
personal_rehabilitation_yn=0 & personal_rehabilitation_complete_yn=2
(개인회생자가 아니면서 결측) # check
-> 5901578
personal_rehabilitation_yn=1 & personal_rehabilitation_complete_yn=0
(개인회생자이면서 납입중)
-> 34550
personal_rehabilitation_yn=1 & personal_rehabilitation_complete_yn=1
(개인회생자이면서 납입완료)
-> 6627
personal_rehabilitation_yn=1 & personal_rehabilitation_complete_yn=2
-> 0
personal_rehabilitation_yn=2 & personal_rehabilitation_complete_yn=2
(둘 다 결측치) # check
-> 5885910

나머지의 경우 0
```

```python
#요일 지정하기
import datetime
train_set['time'] = pd.to_datetime(train_set['loanapply_insert_time'])
train_set['weekday'] = train_set['time'].dt.weekday
train_set['weekday'].value_counts() #0~4 평일, 5~6 주말
train_set['week'] = train_set['weekday'].apply(lambda x : 0 if x <5 else 1)
train_set.drop(['time','weekday'], axis=1, inplace=True) #평일 주말

내가 했을 땐 0.4로 거의 동일하게 나옴
```

```python
import numpy as np
condition=[(data['personal_rehabilitation_yn']==0)&(data['personal_rehabilitation_complete_yn']==0),
           (data['personal_rehabilitation_yn']==0)&(data['personal_rehabilitation_complete_yn']==1),
           (data['personal_rehabilitation_yn']==0)&(data['personal_rehabilitation_complete_yn']==2),
           (data['personal_rehabilitation_yn']==1)&(data['personal_rehabilitation_complete_yn']==0),
           (data['personal_rehabilitation_yn']==1)&(data['personal_rehabilitation_complete_yn']==1),
           (data['personal_rehabilitation_yn']==1)&(data['personal_rehabilitation_complete_yn']==2)]
choicelist=[1,2,3,4,5,6]
data['personal'] = np.select(condition,choicelist)

data['personal'].value_counts() 에서 데이터수가 16개 이고 개인회생자가 아닌데 납입중이라서 삭제

idx = data[data['personal']==2].index
data.drop(idx,inplace=True)
# 2번 날린것 확인
data['personal'].value_counts()
# 개인회생 더미변수
import pandas as pd
data = pd.get_dummies(data,columns=['personal'])
```

[is_applied 예측.ipynb]((1)%20d82ea6124879491d8f29f61d12819f3c/is_applied_%25EC%2598%2588%25EC%25B8%25A1.ipynb)

[승부는 미궁속으로 (1).ipynb]((1)%20d82ea6124879491d8f29f61d12819f3c/%25EC%258A%25B9%25EB%25B6%2580%25EB%258A%2594_%25EB%25AF%25B8%25EA%25B6%2581%25EC%2586%258D%25EC%259C%25BC%25EB%25A1%259C_(1).ipynb)

[승부는 미궁속으로 (1).ipynb]((1)%20d82ea6124879491d8f29f61d12819f3c/%25EC%258A%25B9%25EB%25B6%2580%25EB%258A%2594_%25EB%25AF%25B8%25EA%25B6%2581%25EC%2586%258D%25EC%259C%25BC%25EB%25A1%259C_(1)%201.ipynb)

[https://data-newbie.tistory.com/257](https://data-newbie.tistory.com/257)